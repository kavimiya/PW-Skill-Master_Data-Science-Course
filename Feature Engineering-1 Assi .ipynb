{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6b225f-1b3a-49f1-ae52-d3ef55e92bb9",
   "metadata": {},
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some  algorithms that are not affected by missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3227fd6-d360-4ec5-959f-b23aa618b28f",
   "metadata": {},
   "source": [
    "## What Are Missing Values?\n",
    "* Missing values in a dataset occur when some data points are not recorded or are absent. They can appear in different ways, such as:\n",
    "\n",
    "* Empty Cells: Cells in a dataset that have no value.\n",
    "* NA/NaN: Special markers used to indicate missing values.\n",
    "* Placeholder Values: Unusual values like -9999 or \"Unknown\" used to represent missing data.\n",
    "\n",
    "## Why Is It Essential to Handle Missing Values?\n",
    " * Data Integrity: Missing values can lead to incorrect or biased analysis if not addressed.\n",
    "* Model Accuracy: Many algorithms require complete data to make accurate predictions. Missing values can reduce model performance or lead to errors.\n",
    "* Statistical Methods: Missing values can affect statistical calculations, such as means and variances, making results unreliable.\n",
    "\n",
    "## How to Handle Missing Values\n",
    "* Imputation: Fill missing values with estimates, such as mean, median, or mode.\n",
    "* Deletion: Remove rows or columns with missing values.\n",
    "* Predictive Modeling: Use machine learning models to predict and fill in missing values.\n",
    "\n",
    "\n",
    "## Algorithms Not Affected by Missing Values\n",
    "* Some algorithms can handle missing values directly and do not require imputation before training:\n",
    "* Decision Trees: Can handle missing values by splitting data based on available values.\n",
    "* Random Forests: Like decision trees, they handle missing values well due to their ensemble nature.\n",
    "* k-Nearest Neighbors (k-NN): Can use available features to find nearest neighbors even if some features are missing.\n",
    "* XGBoost: A popular gradient boosting algorithm that has built-in methods for dealing with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011ce66-5071-4aee-9253-9e0a6aec10d3",
   "metadata": {},
   "source": [
    "# Q2: List down techniques used to handle missing data.  Give an example of each with python code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "689c022d-3f7f-4918-a850-b967f9fdd1a1",
   "metadata": {},
   "source": [
    "## 1. Imputation\n",
    "### Filling with Mean, Median, or Mode:\n",
    "\n",
    "* Mean Imputation: Replace missing values with the mean of the column.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5]})\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['A'] = imputer.fit_transform(df[['A']])\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "## Median Imputation: Replace missing values with the median of the column.\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df['A'] = imputer.fit_transform(df[['A']])\n",
    "print(df)\n",
    "\n",
    "\n",
    "## Mode Imputation: Replace missing values with the mode (most frequent value).\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['A'] = imputer.fit_transform(df[['A']])\n",
    "print(df)\n",
    "\n",
    "## 2. Forward Fill and Backward Fill\n",
    "## Forward Fill: Fill missing values using the last known value.\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'A': [1, None, None, 4, 5]})\n",
    "df['A'] = df['A'].fillna(method='ffill')\n",
    "print(df)\n",
    "\n",
    "## Backward Fill: Fill missing values using the next known value.\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'A': [1, None, None, 4, 5]})\n",
    "df['A'] = df['A'].fillna(method='bfill')\n",
    "print(df)\n",
    "\n",
    "\n",
    "## 3. Deletion\n",
    "## Remove Rows with Missing Values:\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5]})\n",
    "df_cleaned = df.dropna()\n",
    "print(df_cleaned)\n",
    "\n",
    "## Remove Columns with Missing Values:\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5], 'B': [None, None, None, 8, 9]})\n",
    "df_cleaned = df.dropna(axis=1)\n",
    "print(df_cleaned)\n",
    "\n",
    "4. Predictive Modeling\n",
    "* Using k-Nearest Neighbors (k-NN) for Imputation:\n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample array with missing values\n",
    "X = np.array([[1, 2, np.nan],\n",
    "              [3, 4, 5],\n",
    "              [5, np.nan, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Impute missing values using k-NN\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "print(X_imputed)\n",
    "\n",
    "## 5. Multiple Imputation\n",
    "* Using IterativeImputer (multiple imputation):\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5],\n",
    "                   'B': [None, 1, 2, 3, 4]})\n",
    "\n",
    "# Impute missing values using IterativeImputer\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_imputed)\n",
    "\n",
    "\n",
    "## 6. Using Algorithms That Handle Missing Values Directly\n",
    "## Using XGBoost:\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample data\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.loc[0, 'sepal length (cm)'] = None  # Introduce missing value\n",
    "\n",
    "# Define XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Train the model (XGBoost handles missing values automatically)\n",
    "model.fit(df, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b42b64-d4ac-4937-b5ac-184c7f9dae68",
   "metadata": {},
   "source": [
    "# Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cc508-fe2f-47ec-8fa7-d46ffc9e3ce6",
   "metadata": {},
   "source": [
    "## What is Imbalanced Data?\n",
    "* Imbalanced data occurs when one class in your dataset is much more common than another. For example, if you’re detecting fraud and only 1% of transactions are fraud, your data is imbalanced.\n",
    "\n",
    "## Problems with Imbalanced Data\n",
    "* Model Bias: The model might mostly predict the majority class and ignore the minority class.\n",
    "* Misleading Accuracy: High accuracy doesn’t mean good performance if the model fails on the minority class.\n",
    "* Poor Detection: Important but rare events (like fraud) may not be detected.\n",
    "\n",
    "## Handling Imbalanced Data\n",
    "* Resampling: Adjust the number of examples in each class.\n",
    "* Oversampling: Add more examples of the minority class.\n",
    "* Undersampling: Remove some examples from the majority class.\n",
    "* Adjust Class Weights: Make the model pay more attention to the minority class.\n",
    "* Evaluation Metrics: Use metrics like Precision, Recall, or F1-Score instead of just accuracy to better understand model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc5b0b-77de-47e5-a9e7-777be4e51ad9",
   "metadata": {},
   "source": [
    "# Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f3237-d188-44a4-b073-f0fab9dac93f",
   "metadata": {},
   "source": [
    "## Up-sampling\n",
    "### What It Is:\n",
    "\n",
    "* Up-sampling is the process of increasing the number of examples in the minority class to balance the dataset.\n",
    "\n",
    "## When It's Required:\n",
    "\n",
    "* When the dataset is imbalanced, and the minority class has too few examples compared to the majority class.\n",
    "## Example:\n",
    "\n",
    "* If you have 1000 examples of class A and only 100 examples of class B, up-sampling class B would involve creating additional synthetic examples or duplicating existing ones to increase the number of examples in class B.\n",
    "\n",
    "## Down-sampling\n",
    "## What It Is:\n",
    "\n",
    "* Down-sampling is the process of reducing the number of examples in the majority class to balance the dataset.\n",
    "\n",
    "## When It's Required:\n",
    "\n",
    "* When the dataset is imbalanced, and the majority class has too many examples compared to the minority class.\n",
    "\n",
    "## Example:\n",
    "\n",
    "* If you have 1000 examples of class A and only 100 examples of class B, down-sampling class A would involve randomly removing some examples from class A to match the number of examples in class B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c462f6b-2224-486f-97ac-d15511724b38",
   "metadata": {},
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7829d-2a09-4fad-8c52-108a04c3330f",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "## What It Is:\n",
    "\n",
    "* Data augmentation involves creating new training samples from the existing data by applying various transformations. This helps increase the diversity of the data and improve the model’s performance.\n",
    "\n",
    "## Example:\n",
    "\n",
    "* For images, data augmentation might include rotating, flipping, or zooming the images. For text, it could involve paraphrasing or adding noise.\n",
    "\n",
    "## SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "## What It Is:\n",
    "\n",
    "* SMOTE is a technique for up-sampling the minority class by creating synthetic samples. It generates new examples by interpolating between existing minority class examples.\n",
    "## How It Works:\n",
    "\n",
    "* SMOTE picks a minority class example and creates new examples by combining it with its nearest neighbors. This helps to balance the classes without simply duplicating existing data.\n",
    "## Example:\n",
    "\n",
    "* If you have a dataset with 100 examples of a rare class, SMOTE might create 200 new synthetic examples by interpolating between these 100 existing ones, making the dataset more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cefbf71-b036-435c-bea6-4d59cb5911f5",
   "metadata": {},
   "source": [
    "# Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c3dd4-66d9-40dc-a60d-6b360dd4107c",
   "metadata": {},
   "source": [
    "## What Are Outliers?\n",
    "### Definition:\n",
    "\n",
    "* Outliers are data points that are significantly different from the majority of the data. They are unusually high or low compared to other values in the dataset.\n",
    "### Example:\n",
    "\n",
    "* In a dataset of people’s ages, if most people are between 20 and 40 years old, a few individuals aged 100 might be considered outliers.\n",
    "\n",
    "## Why Is It Essential to Handle Outliers?\n",
    "## Distortion of Analysis:\n",
    "\n",
    "* Outliers can skew the results of statistical calculations, like the mean, leading to misleading conclusions.\n",
    "\n",
    "## Impact on Models:\n",
    "\n",
    "* Outliers can affect the performance of machine learning models, causing them to make inaccurate predictions.\n",
    "\n",
    "\n",
    "## Detection of Errors:\n",
    "\n",
    "* Sometimes, outliers indicate errors or anomalies in data collection that need to be corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed0b7d-57ec-4b0b-ae54-cc013d463535",
   "metadata": {},
   "source": [
    "# Q7: You are working on a project that requires analyzing customer data. However, you notice that some of  the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff06752-00da-41f2-911b-93d58fabd084",
   "metadata": {},
   "source": [
    "## Imputation:\n",
    "\n",
    "* Mean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column.\n",
    "* Predictive Imputation: Use machine learning models to estimate missing values based on other data.\n",
    "\n",
    "##  Resampling:\n",
    "\n",
    "* Forward Fill/Backward Fill: Fill missing values using the last or next available data.\n",
    "\n",
    "## Deletion:\n",
    "\n",
    "* Remove Rows: Exclude rows with missing values.\n",
    "* Remove Columns: Exclude columns with too many missing values.\n",
    "\n",
    "## Flagging:\n",
    "\n",
    "* Create Indicators: Add a new column to indicate whether the value was missing, helping to preserve information about the missingness.\n",
    "## Multiple Imputation:\n",
    "\n",
    "* Iterative Imputation: Use multiple models to predict and fill missing values, accounting for uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0dcb0-5beb-424c-b88e-0934127b5539",
   "metadata": {},
   "source": [
    "# Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are  some strategies you can use to determine if the missing data is missing at random or if there is a pattern  to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b5f04-a7a0-4241-8c8b-ec0164ec6393",
   "metadata": {},
   "source": [
    "## Visual Inspection:\n",
    "\n",
    "* Missing Data Matrix: Use a heatmap or matrix plot to visualize the missing data and identify any patterns.\n",
    "* Correlation Analysis: Plot missing data against other variables to see if there are correlations.\n",
    "\n",
    "## Statistical Tests:\n",
    "\n",
    "* Little's MCAR Test: Perform statistical tests to check if data is missing completely at random (MCAR).\n",
    "* Chi-Square Test: Test for independence between missingness and other variables to see if missing data is related to certain factors.\n",
    "\n",
    "## Compare Distributions:\n",
    "\n",
    "* Analyze Missing vs. Non-Missing Groups: Compare the distribution of other variables between records with missing values and those without to check for differences.\n",
    "\n",
    "## Data Patterns:\n",
    "\n",
    "* Group Analysis: Examine if missing data occurs more frequently in specific groups or conditions within your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d01ff2-6012-4561-87c1-e75a7bf59c52",
   "metadata": {},
   "source": [
    "# Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the  dataset do not have the condition of interest, while a small percentage do. What are some strategies you  can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba2b30-e166-4f8e-8abc-a55dc70e3d46",
   "metadata": {},
   "source": [
    "## Confusion Matrix:\n",
    "\n",
    "* Evaluate the true positives, true negatives, false positives, and false negatives to understand how well the model is distinguishing between classes.\n",
    "\n",
    "## Precision, Recall, and F1-Score:\n",
    "\n",
    "* Focus on metrics like Precision (positive predictive value), Recall (sensitivity), and F1-Score, which provide a better understanding of performance on the minority class.\n",
    "\n",
    "## ROC-AUC Score:\n",
    "\n",
    "* Use the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) to measure the model's ability to discriminate between classes.\n",
    "\n",
    "## Precision-Recall Curve:\n",
    "\n",
    "* Plot the Precision-Recall curve to evaluate model performance specifically on the minority class.\n",
    "\n",
    "## Cross-Validation:\n",
    "\n",
    "* Apply stratified cross-validation to ensure each fold has a representative ratio of classes, providing a more robust performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689985dc-f70c-4612-b689-76503bd6b26d",
   "metadata": {},
   "source": [
    "# Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is  unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to  balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52d6b097-2f99-479d-a0ff-50a7c30e121c",
   "metadata": {},
   "source": [
    "# Random Down-Sampling:\n",
    "\n",
    "Remove Samples: Randomly reduce the number of samples in the majority class to match the minority class.\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assume df is your DataFrame and 'satisfaction' is the target column\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'not_satisfied']\n",
    "\n",
    "# Down-sample majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                replace=False, \n",
    "                                n_samples=len(minority_class), \n",
    "                                random_state=42)\n",
    "\n",
    "# Combine with minority class\n",
    "df_balanced = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "# Stratified Sampling:\n",
    "\n",
    "## Ensure Representation:\n",
    "* Use stratified sampling to ensure balanced class representation in training and testing sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7cead-0fd8-42c4-9a67-188937702275",
   "metadata": {},
   "source": [
    "# Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a  project that requires you to estimate the occurrence of a rare event. What methods can you employ to  balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdc502a5-ca82-4c5a-809f-b0fe0e0490d2",
   "metadata": {},
   "source": [
    "# SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "Generate Synthetic Samples: Create new synthetic samples for the minority class by interpolating between existing examples.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "Random Over-Sampling:\n",
    "\n",
    "Duplicate Samples: Randomly duplicate samples from the minority class to increase its representation.\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "minority_class = df[df['target'] == 1]\n",
    "majority_class = df[df['target'] == 0]\n",
    "\n",
    "# Up-sample minority class\n",
    "minority_upsampled = resample(minority_class, \n",
    "                              replace=True, \n",
    "                              n_samples=len(majority_class), \n",
    "                              random_state=42)\n",
    "\n",
    "# Combine with majority class\n",
    "df_balanced = pd.concat([majority_class, minority_upsampled])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
